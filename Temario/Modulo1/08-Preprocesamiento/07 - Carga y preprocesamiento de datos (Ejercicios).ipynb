{"cells":[{"cell_type":"markdown","metadata":{"id":"7_hr06ahXKd7"},"source":["# 7 - Carga y preprocesamiento de datos (Ejercicios)\n","\n","En este apartado, hemos estudiado uno de los componentes más importantes del ciclo de vida de los modelos de inteligencia artificial: la adquisición y manipulación de los datos. En los conjuntos de datos propuestos, tendremos por una parte la información relativa a los atributos de la muestra, y por otra los nombres de los mismos. Tu tarea será:\n","\n","* Importar desde consola por comandos Linux los ficheros oportunos.\n","* Leer los ficheros `data` y `names`.\n","* Explorar el fichero `names` para analizar qué tipo de expresiones regulares necesitas para identificar los nombres de las columnas en la metadata.\n","* Aplicar las transformaciones *regex* pertinentes y obtener los nombres de las columnas para construir los datos. *Pista: El nombre de la variable respuesta tendremos que añadirlo al final ya que no viene explícitamente citado*.\n","* Realizar un conveniente preprocesmiento de las variables en función de su tipo."]},{"cell_type":"markdown","metadata":{"id":"vd83apIaZtHH"},"source":["## 7.1 - *Adult* [dataset](https://archive.ics.uci.edu/ml/datasets/Adult)\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"ispLepCMZOoG"},"outputs":[],"source":["import os\n","import re\n","import pandas as pd\n","import numpy as np"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"-CJ2qGh9XBwJ"},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'c:\\\\content\\\\adult_dataset\\\\adult.data'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[6], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# # Creamos una carpeta para que contenga a nuestro dataset\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# !mkdir /content/adult_dataset\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# # Movemos el directorio activo a esa localización\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# !wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Leemos datos\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetcwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43madult.data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     11\u001b[0m     data \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39msplitlines() \u001b[38;5;66;03m# Dividimos el texto por saltos de línea\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     data \u001b[38;5;241m=\u001b[39m [elem\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m data] \u001b[38;5;66;03m# Dividimos cada línea por las comas y removemos líneas vacías\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Borja\\Documents\\GitHub\\Ejercicios-clase1\\entorno\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'c:\\\\content\\\\adult_dataset\\\\adult.data'"]}],"source":["# Creamos una carpeta para que contenga a nuestro dataset\n","!mkdir /content/adult_dataset\n","# Movemos el directorio activo a esa localización\n","%cd /content/adult_dataset\n","# Descargamos el fichero que contiene los datos a nuestro directorio activo\n","!wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\n","# Descargamos la metadata asociada al conjunto de datos\n","!wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names\n","# Leemos datos\n","with open(os.path.join(os.getcwd(),'adult.data'),'r') as f:\n","    data = f.read().splitlines() # Dividimos el texto por saltos de línea\n","    data = [elem.split(',') for elem in data] # Dividimos cada línea por las comas y removemos líneas vacías\n","# Leemos metadata\n","with open(os.path.join(os.getcwd(),'adult.names'),'r') as f:\n","    metadata = f.read().splitlines()\n","# Regex\n","regex_fn = lambda text: re.findall('^[a-zA-Z-]+:{1}', text)\n","reg_text_fn = lambda text : re.findall('[a-zA-Z- ]+', text)\n","metadata_list = [regex_fn(elem)[0] for elem in metadata if regex_fn(elem)]\n","col_names = [reg_text_fn(elem)[0] for elem in metadata_list if reg_text_fn(elem)] + ['label']\n","# Construimos el objeto pd.DataFrame\n","df = pd.DataFrame(data=data, columns=col_names)"]},{"cell_type":"markdown","metadata":{"id":"THZ2TxfIcaBE"},"source":["## 7.2 - Beijing Multi-Site [Air Quality Data](https://archive.ics.uci.edu/ml/datasets/Beijing+Multi-Site+Air-Quality+Data)\n","\n","En este conjunto de datos no tendremos que hacer un esfuerzo muy grande en lo relativo a estudiar la *metadata*, pero exploraremos una serie de comandos de Linux que nos será muy útil conocer:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bra5pAQQc1Pf"},"outputs":[],"source":["# Movemos el directorio activo a una nueva localización para este dataset\n","## Retrocedemos un nivel\n","%cd ..\n","## Creamos carpeta\n","!mkdir /content/air_quality_dataset\n","## Movemos directorio activo\n","%cd /content/air_quality_dataset\n","# Descargamos fichero comprimido\n","!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00501/PRSA2017_Data_20130301-20170228.zip\n","# Descargamos el fichero que contiene los datos a nuestro directorio activo\n","!unzip PRSA2017_Data_20130301-20170228.zip\n","# Nos movemos a la carpeta que contenía el zip\n","%cd PRSA_Data_20130301-20170228"]},{"cell_type":"markdown","metadata":{"id":"WavKUb55hd_2"},"source":["Ahora te toca, ¿eres capaz de leer todos los `csv`, concatenarlos y construir un `pd.DataFrame` en una sola línea de código?\n","\n","\n","```python\n","df = pd.concat([pd.read_csv(elem) for elem in os.listdir()]).reset_index(drop=True)\n","```"]},{"cell_type":"markdown","metadata":{"id":"DH3nPI72jN5z"},"source":["## 7.3 - Solar flare [dataset](https://archive.ics.uci.edu/ml/datasets/Solar+Flare)\n","\n","En este conjunto de datos, tendremos dos ficheros relativos a `data`, cuya primera fila serán las especificaciones temporales, por lo que deberemos quitarla, y además en los registros de datos las variables no vienen delimitadas por `','`, si no por espacios en blanco:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NUEpFJ5mjRlG"},"outputs":[],"source":["# Movemos el directorio activo a una nueva localización para este dataset\n","## Retrocedemos dos niveles\n","%cd ..\n","%cd ..\n","## Creamos carpeta\n","!mkdir /content/solar_flare_dataset\n","## Movemos directorio activo\n","%cd /content/solar_flare_dataset\n","# Descargamos los ficheros que contienen los datos a nuestro directorio activo\n","!wget https://archive.ics.uci.edu/ml/machine-learning-databases/solar-flare/flare.data1\n","!wget https://archive.ics.uci.edu/ml/machine-learning-databases/solar-flare/flare.data2\n","# Descargamos la metadata asociada al conjunto de datos\n","!wget https://archive.ics.uci.edu/ml/machine-learning-databases/solar-flare/flare.names\n","# Leemos datos\n","## Leemos primer fichero de datos\n","with open(os.path.join(os.getcwd(),'flare.data1'),'r') as f:\n","    data1 = f.read().splitlines() # Dividimos el texto por saltos de línea\n","    data1 = [elem.split(' ') for elem in data1 if elem!=''] # Dividimos cada línea por las comas y removemos líneas vacías\n","    data1 = data1[1:] # Quitamos la línea de metadata temporal\n","## Leemos segundo fichero de datos\n","with open(os.path.join(os.getcwd(),'flare.data2'),'r') as f:\n","    data2 = f.read().splitlines() # Dividimos el texto por saltos de línea\n","    data2 = [elem.split(' ') for elem in data2 if elem!=''] # Dividimos cada línea por las comas y removemos líneas vacías\n","    data2 = data2[1:] # Quitamos la línea de metadata temporal\n","## Combinamos ambas listas\n","data = data1+data2\n","# Leemos metadata\n","with open(os.path.join(os.getcwd(),'flare.names'),'r') as f:\n","    metadata = f.read().splitlines()\n","## Regex\n","regex_fn = lambda text: re.findall('^\\s+[0-9]+\\.{1}\\s{1}[a-zA-Z- ]+', text)\n","reg_text_fn = lambda text : re.findall('[a-zA-Z-]+', text)\n","metadata_list = [regex_fn(elem)[0].strip() for elem in metadata if regex_fn(elem)]\n","col_names = [reg_text_fn(elem)[0] for elem in metadata_list if reg_text_fn(elem)]\n","# Construimos el objeto pd.DataFrame\n","df = pd.DataFrame(data=data, columns=col_names)"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"07 - Carga y preprocesamiento de datos (Ejercicios).ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":0}
